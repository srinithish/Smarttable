# -*- coding: utf-8 -*-
"""TrainImageClassifier_GE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oe1ix3coQ9x4-nPDvLqNiEojGRGiQk6z
"""

import tensorflow as tf
import numpy as np
import sklearn
import pandas as pd
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

import makeInference
import pickle

import os

import cv2 as cv
import time
from sklearn.model_selection import train_test_split

isTrain = True




workDir = "./"

dictXnYPath = workDir+'Data/AllObjectLevelData.pkl'

dictOfXandY = pickle.load(open(dictXnYPath, 'rb'))

X = tf.placeholder(tf.float32,[None,28,28,3],name = "X")
Y  =  tf.placeholder(tf.float32,[None,2], name = "Y")

### layer configuration

layer1 = { "type":"conv2d",
            "filters":16,
            "kernel_size":[4,4],
            "activation": tf.nn.leaky_relu
         }

layer2 = { "type":"maxpool",
            "pool_size":3,
            "padding":'valid'
         }


layer3 = { "type":"conv2d",
            "filters":32,
            "kernel_size":[4,4],
          "activation": tf.nn.leaky_relu
         }


layer4 = { "type":"fullyConnected",
            'outputUnits': 2,
            
            "activation": None
          }

layers = [layer1,layer2,layer3,layer4]

def get_network_output(input_x,layers):
    '''
    
    inputs = input_x 
    output = latent_vectors
    
    input.shape => (batch_size,28,28)  // We need to reshape to add filters dim
    output.shape => (batch_size,6)  //6 values corresponding to 3 means and 3 sd
    '''
    constructed_network = []
    
    # He initialization
    initializer = tf.keras.initializers.he_normal()
        
    for layer in layers:
      
        if len(constructed_network) == 0: # This is the First layer
            this_input = input_x
        else:
            this_input = constructed_network[-1]
   
      
        if layer["type"] == "conv2d":
            layerOutput = tf.keras.layers.Conv2D( 
                         
                         filters = layer["filters"],
                         kernel_size = layer["kernel_size"],
                         strides = 1,
                         padding = "same",
                         kernel_initializer = initializer,
                         activation = layer["activation"]
                        )(this_input)
        
        elif layer["type"] == "maxpool":
            layerOutput = tf.keras.layers.MaxPool2D(
                         
                          pool_size = layer["pool_size"],
                          strides = layer["pool_size"], # Same as pool size to not consider the same box twice
                          padding='valid')(this_input)
        
        
        elif layer['type'] == 'fullyConnected':
          
            this_input = tf.keras.layers.Flatten()(this_input) ##flatten input
          
            layerOutput = tf.keras.layers.Dense(
                              units = layer['outputUnits'],
                              activation = layer["activation"],
                              kernel_initializer= initializer)(this_input)

            
        
        # Push this layer to network
        constructed_network.append(layerOutput)
      
    return constructed_network[-1]

"""### Loss and minimisation"""

networkOutput = get_network_output(X,layers)

lossCalcu  = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=networkOutput, labels=Y))

gradOptimizer = tf.train.AdamOptimizer(learning_rate=0.0003)

train = gradOptimizer.minimize(lossCalcu)

"""
Evaluations and prdictions
"""
probPredictions = tf.nn.softmax(networkOutput)
labelPreds = tf.argmax(networkOutput, 1)
correct_pred = tf.equal(tf.argmax(networkOutput, 1), tf.argmax(Y, 1))

accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
initialise = tf.global_variables_initializer()

epochs = 1000
learning_rate = 0.003
print_step = 1
save_step = 1
batchSize = 100
save_dir = workDir+'saved_models/'

sess = tf.InteractiveSession()
sess.run(initialise)

saver = tf.train.Saver()


#saver.restore(sess, save_dir+"model.ckpt")

xData = np.array(dictOfXandY['X'])
yData = tf.one_hot(np.array(dictOfXandY['Y']),depth = len(set(dictOfXandY['Y']))).eval()

x_train, x_test, y_train, y_test = train_test_split(xData, yData, test_size=0.1, random_state=42)


if isTrain == True:
    for epoch in range(epochs):
    
        for iteration, offset in enumerate(range(0, len(x_train), batchSize)):
            
            x_batch, yBatch = x_train[offset: offset + batchSize], y_train[offset: offset + batchSize]
        
            feedTrain = {X:x_batch,Y:yBatch}
    
            sess.run(train,feed_dict = feedTrain)
    
            
        feedTest = {X:x_test,Y:y_test}
        loss, acc = sess.run([lossCalcu, accuracy], feed_dict=feedTest) 
            
            
        if epoch%print_step == 0:
            
          print("Step " + str(epoch) + ", Loss= " + str(loss) + ", Testing Accuracy= " + str(acc))
        
        if epoch%save_step == 0:
            
            save_path = saver.save(sess, save_dir+"model.ckpt")







if isTrain == False:
    vidCapHandle = makeInference.initVidCap(camNum=1)
    
    while True:
        
        img = makeInference.getFrame(vidCapHandle,mirror=False)
        imgReshaped = np.reshape(img,(1,300,300,3))
        
        if cv.waitKey(1) == 27: 
                break
            
        Label = sess.run(labelPreds,feed_dict={X:imgReshaped})
        
        makeInference.showFrame(img,Label)
        
    cv.destroyAllWindows()
    vidCapHandle.release()   
    
    
    
for i in range(len(x_test)):
    img = x_test[:3]
    imgReshaped = np.reshape(img,(1,28,28,3))
    
    Label = sess.run(labelPreds,feed_dict={X:img})
    plt.title(Label)
    plt.imshow(img)
    plt.pause(5)
    plt.close()